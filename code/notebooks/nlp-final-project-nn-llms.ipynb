{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Description\n",
    "In this notebook I've used Gemma-2-9b-it model to paraphrase the math problem classification data from\n",
    "[this](https://www.kaggle.com/competitions/classification-of-math-problems-by-kasut-academy/overview) Kaggle competition.\n",
    "\n",
    "### What this Notebook has:\n",
    "- Uses vllm to load the model.\n",
    "- Run inference to generate paraphrased data from the original data.\n",
    "- Save the data.\n",
    "\n",
    "### Next steps for this exploration:\n",
    "- Better analysis of the original vs the paraphrased data.\n",
    "- Analyze how much of the actual meaning is carried over to the paraphrased version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T16:03:38.250988Z",
     "iopub.status.busy": "2025-04-25T16:03:38.250726Z",
     "iopub.status.idle": "2025-04-25T16:06:56.040344Z",
     "shell.execute_reply": "2025-04-25T16:06:56.039675Z",
     "shell.execute_reply.started": "2025-04-25T16:03:38.250969Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.1/294.1 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.4/98.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.3/343.3 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.7/913.7 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
      "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
      "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
      "ydata-profiling 4.16.1 requires numba<=0.61,>=0.56.0, but you have numba 0.61.2 which is incompatible.\n",
      "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install vllm -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T16:07:13.441524Z",
     "iopub.status.busy": "2025-04-25T16:07:13.440913Z",
     "iopub.status.idle": "2025-04-25T16:07:36.113065Z",
     "shell.execute_reply": "2025-04-25T16:07:36.112510Z",
     "shell.execute_reply.started": "2025-04-25T16:07:13.441498Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-25 16:07:21 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-25 16:07:23.517765: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745597243.720718      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745597243.780592      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Quantized Gemma Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T16:09:49.301153Z",
     "iopub.status.busy": "2025-04-25T16:09:49.300859Z",
     "iopub.status.idle": "2025-04-25T16:12:32.755396Z",
     "shell.execute_reply": "2025-04-25T16:12:32.754534Z",
     "shell.execute_reply.started": "2025-04-25T16:09:49.301130Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b0de3635c44c5a8eefa1dcc2448d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-25 16:09:49 [config.py:2836] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 04-25 16:10:02 [config.py:689] This model supports multiple tasks: {'classify', 'reward', 'score', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 04-25 16:10:03 [config.py:768] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "WARNING 04-25 16:10:03 [arg_utils.py:1731] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "INFO 04-25 16:10:03 [config.py:1713] Defaulting to use mp for distributed inference\n",
      "INFO 04-25 16:10:03 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='hugging-quants/gemma-2-9b-it-AWQ-INT4', speculative_config=None, tokenizer='hugging-quants/gemma-2-9b-it-AWQ-INT4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=hugging-quants/gemma-2-9b-it-AWQ-INT4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359a6477786b4c539159af4e60f75870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ff4501ac394c2a8e5aacdb8100bcac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2abb04c5113484385478183fa03dbc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008dabe872ab40748434f38376a3d917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1abef13d45457dae0a9341bc76d675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-25 16:10:06 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=151)\u001b[0;0m INFO 04-25 16:10:06 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\n",
      "INFO 04-25 16:10:06 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 04-25 16:10:06 [cuda.py:289] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=151)\u001b[0;0m INFO 04-25 16:10:06 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=151)\u001b[0;0m INFO 04-25 16:10:06 [cuda.py:289] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W425 16:10:18.677615205 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W425 16:10:18.190375030 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W425 16:10:28.688091642 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-25 16:10:38 [utils.py:993] Found nccl from library libnccl.so.2\n",
      "INFO 04-25 16:10:38 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=151)\u001b[0;0m INFO 04-25 16:10:38 [utils.py:993] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=151)\u001b[0;0m INFO 04-25 16:10:38 [pynccl.py:69] vLLM is using nccl==2.21.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W425 16:10:38.698647361 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-25 16:10:38 [custom_all_reduce_utils.py:206] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 04-25 16:11:01 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=151)\u001b[0;0m INFO 04-25 16:11:01 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 04-25 16:11:01 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_67e9756a'), local_subscribe_addr='ipc:///tmp/5d7688b9-ec4e-40d5-8ab0-50b2427234dd', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 04-25 16:11:01 [parallel_state.py:959] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorkerProcess pid=151)\u001b[0;0m INFO 04-25 16:11:01 [parallel_state.py:959] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "INFO 04-25 16:11:01 [model_runner.py:1110] Starting to load model hugging-quants/gemma-2-9b-it-AWQ-INT4...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=151)\u001b[0;0m INFO 04-25 16:11:01 [model_runner.py:1110] Starting to load model hugging-quants/gemma-2-9b-it-AWQ-INT4...\n",
      "WARNING 04-25 16:11:01 [logger.py:63] XFormers does not support logits soft cap. Outputs may be slightly off.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=151)\u001b[0;0m WARNING 04-25 16:11:01 [logger.py:63] XFormers does not support logits soft cap. Outputs may be slightly off.\n",
      "INFO 04-25 16:11:01 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=151)\u001b[0;0m INFO 04-25 16:11:01 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53332ab57acd4a8ea111b88dacebed48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8de2be66e54930a933b68269c56941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.16G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-25 16:11:15 [weight_utils.py:281] Time spent downloading weights for hugging-quants/gemma-2-9b-it-AWQ-INT4: 14.237964 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05451a13168441b4bded1ef45062ffd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/87.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b65ccbd02954e61a8f2a7b033c7f8dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=151)\u001b[0;0m INFO 04-25 16:11:19 [weight_utils.py:281] Time spent downloading weights for hugging-quants/gemma-2-9b-it-AWQ-INT4: 3.121956 seconds\n",
      "INFO 04-25 16:11:21 [loader.py:458] Loading weights took 5.98 seconds\n",
      "INFO 04-25 16:11:22 [model_runner.py:1146] Model loading took 2.8943 GiB and 20.748736 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=151)\u001b[0;0m INFO 04-25 16:11:22 [loader.py:458] Loading weights took 3.64 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=151)\u001b[0;0m INFO 04-25 16:11:23 [model_runner.py:1146] Model loading took 2.8943 GiB and 21.611032 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=151)\u001b[0;0m INFO 04-25 16:11:37 [worker.py:267] Memory profiling takes 14.32 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=151)\u001b[0;0m INFO 04-25 16:11:37 [worker.py:267] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=151)\u001b[0;0m INFO 04-25 16:11:37 [worker.py:267] model weights take 2.89GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 0.66GiB; the rest of the memory reserved for KV Cache is 9.59GiB.\n",
      "INFO 04-25 16:11:38 [worker.py:267] Memory profiling takes 14.51 seconds\n",
      "INFO 04-25 16:11:38 [worker.py:267] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\n",
      "INFO 04-25 16:11:38 [worker.py:267] model weights take 2.89GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 2.38GiB; the rest of the memory reserved for KV Cache is 7.87GiB.\n",
      "INFO 04-25 16:11:38 [executor_base.py:112] # cuda blocks: 3069, # CPU blocks: 1560\n",
      "INFO 04-25 16:11:38 [executor_base.py:117] Maximum concurrency for 8192 tokens per request: 5.99x\n",
      "INFO 04-25 16:11:43 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04dcdde6cfd347059d3037456b27ff77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=151)\u001b[0;0m INFO 04-25 16:11:43 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-25 16:12:32 [custom_all_reduce.py:195] Registering 2975 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=151)\u001b[0;0m INFO 04-25 16:12:32 [custom_all_reduce.py:195] Registering 2975 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=151)\u001b[0;0m INFO 04-25 16:12:32 [model_runner.py:1598] Graph capturing finished in 49 secs, took 0.43 GiB\n",
      "INFO 04-25 16:12:32 [model_runner.py:1598] Graph capturing finished in 49 secs, took 0.43 GiB\n",
      "INFO 04-25 16:12:32 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 69.52 seconds\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"hugging-quants/gemma-2-9b-it-AWQ-INT4\", dtype=\"float16\", quantization='awq', tensor_parallel_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T16:12:32.757361Z",
     "iopub.status.busy": "2025-04-25T16:12:32.757041Z",
     "iopub.status.idle": "2025-04-25T16:12:32.879286Z",
     "shell.execute_reply": "2025-04-25T16:12:32.878395Z",
     "shell.execute_reply.started": "2025-04-25T16:12:32.757333Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/classification-of-math-problems-by-kasut-academy/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/classification-of-math-problems-by-kasut-academy/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create prompts for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T17:45:59.524319Z",
     "iopub.status.busy": "2025-04-25T17:45:59.524008Z",
     "iopub.status.idle": "2025-04-25T17:45:59.545753Z",
     "shell.execute_reply": "2025-04-25T17:45:59.545177Z",
     "shell.execute_reply.started": "2025-04-25T17:45:59.524291Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a an excellent paraphrasing AI assistant. Given a math problem in latex, you will remove unnecessary text like links from it and paraphrase it without changing the actual meaning of the question.\n",
      "\n",
      "Math Problem:\n",
      "4. In a football tournament, only teams from Small Town and Big City participated. There were 9 more teams from Big City than from Small Town. Each team met exactly once, with the winning team getting 1 point, the losing team 0 points, and no draws were possible. Teams from Big City scored 9 times as many points as teams from Small Town. Determine the maximum possible number of wins for the best team from Small Town.\n",
      "\n",
      "The task should be solved independently. You have 210 minutes for solving. The use of notes, literature, or a pocket calculator is not allowed.\n",
      "\n",
      "49th Mathematical Competition for High School Students in Slovenia\n",
      "\n",
      "Velenje, April 16, 2005\n",
      "\n",
      "## Tasks for 4th Year\n",
      "\n",
      "Instructions:\n",
      "**You must only return the paraphrased version of the given math problem and nothing else in your response.**\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "sampling_params_pp = SamplingParams(max_tokens=1000)\n",
    "\n",
    "prompt_template_pp = \"\"\"You are a an excellent paraphrasing AI assistant. Given a math problem in latex, you will remove unnecessary text like links from it and paraphrase it without changing the actual meaning of the question.\n",
    "\n",
    "Math Problem:\n",
    "{problem}\n",
    "\n",
    "Instructions:\n",
    "**You must only return the paraphrased version of the given math problem and nothing else in your response.**\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompts_pp = [prompt_template_pp.format(problem=q) for q in train[\"Question\"]]\n",
    "\n",
    "print(prompts_pp[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Inference of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T17:46:03.024252Z",
     "iopub.status.busy": "2025-04-25T17:46:03.023711Z",
     "iopub.status.idle": "2025-04-25T18:32:15.094008Z",
     "shell.execute_reply": "2025-04-25T18:32:15.093256Z",
     "shell.execute_reply.started": "2025-04-25T17:46:03.024228Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8dff3c6ab38459ba5725b3fc1fd170b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/10189 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-25 17:56:54 [scheduler.py:1769] Sequence group 11822 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n",
      "WARNING 04-25 18:06:33 [scheduler.py:1769] Sequence group 13960 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n",
      "WARNING 04-25 18:17:59 [scheduler.py:1769] Sequence group 16506 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251\n",
      "WARNING 04-25 18:30:07 [scheduler.py:1769] Sequence group 19177 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=301\n"
     ]
    }
   ],
   "source": [
    "output_pp = llm.generate(prompts_pp, sampling_params=sampling_params_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T18:32:23.487087Z",
     "iopub.status.busy": "2025-04-25T18:32:23.486481Z",
     "iopub.status.idle": "2025-04-25T18:46:19.151732Z",
     "shell.execute_reply": "2025-04-25T18:46:19.150771Z",
     "shell.execute_reply.started": "2025-04-25T18:32:23.487061Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f04bdc995c0409989d32b57a1f6c9b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/3044 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-25 18:42:49 [scheduler.py:1769] Sequence group 21917 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=351\n"
     ]
    }
   ],
   "source": [
    "prompts_pp_test = [prompt_template_pp.format(problem=q) for q in test[\"Question\"]]\n",
    "output_pp_test = llm.generate(prompts_pp_test, sampling_params=sampling_params_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T18:48:48.463411Z",
     "iopub.status.busy": "2025-04-25T18:48:48.462646Z",
     "iopub.status.idle": "2025-04-25T18:48:48.493057Z",
     "shell.execute_reply": "2025-04-25T18:48:48.492465Z",
     "shell.execute_reply.started": "2025-04-25T18:48:48.463385Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>label</th>\n",
       "      <th>Question_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A solitaire game is played as follows.  Six di...</td>\n",
       "      <td>3</td>\n",
       "      <td>A solitaire game uses six pairs of unique tile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2. The school table tennis championship was he...</td>\n",
       "      <td>5</td>\n",
       "      <td>In a school table tennis championship using th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Given that $x, y,$ and $z$ are real numbers th...</td>\n",
       "      <td>0</td>\n",
       "      <td>Three real numbers x, y, and z satisfy a serie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$25 \\cdot 22$ Given three distinct points $P\\l...</td>\n",
       "      <td>1</td>\n",
       "      <td>Using three given points P, Q, and R, which ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am thinking of a five-digit number composed ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Consider a five-digit number made up of even d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  label  \\\n",
       "0  A solitaire game is played as follows.  Six di...      3   \n",
       "1  2. The school table tennis championship was he...      5   \n",
       "2  Given that $x, y,$ and $z$ are real numbers th...      0   \n",
       "3  $25 \\cdot 22$ Given three distinct points $P\\l...      1   \n",
       "4  I am thinking of a five-digit number composed ...      5   \n",
       "\n",
       "                                         Question_pp  \n",
       "0  A solitaire game uses six pairs of unique tile...  \n",
       "1  In a school table tennis championship using th...  \n",
       "2  Three real numbers x, y, and z satisfy a serie...  \n",
       "3  Using three given points P, Q, and R, which ha...  \n",
       "4  Consider a five-digit number made up of even d...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Question</th>\n",
       "      <th>Question_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b'Solve 0 = -i - 91*i - 1598*i - 64220 for i.\\n'</td>\n",
       "      <td>Find the value of 'i' that satisfies the equat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Galperin G.A.\\n\\nA natural number $N$ is 999.....</td>\n",
       "      <td>We are looking for a number consisting of $k$ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Example 7 Calculate $\\frac{1}{2 \\sqrt{1}+\\sqrt...</td>\n",
       "      <td>Compute the sum of the following series:\\n\\n1/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>If $A$, $B$, and $C$ represent three distinct ...</td>\n",
       "      <td>Three digits, A, B, and C, each distinct and r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2. Calculate $1+12+123+1234+12345+123456+12345...</td>\n",
       "      <td>Find the hundreds digit of the sum of 1, 12, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           Question  \\\n",
       "0   0   b'Solve 0 = -i - 91*i - 1598*i - 64220 for i.\\n'   \n",
       "1   1  Galperin G.A.\\n\\nA natural number $N$ is 999.....   \n",
       "2   2  Example 7 Calculate $\\frac{1}{2 \\sqrt{1}+\\sqrt...   \n",
       "3   3  If $A$, $B$, and $C$ represent three distinct ...   \n",
       "4   4  2. Calculate $1+12+123+1234+12345+123456+12345...   \n",
       "\n",
       "                                         Question_pp  \n",
       "0  Find the value of 'i' that satisfies the equat...  \n",
       "1  We are looking for a number consisting of $k$ ...  \n",
       "2  Compute the sum of the following series:\\n\\n1/...  \n",
       "3  Three digits, A, B, and C, each distinct and r...  \n",
       "4  Find the hundreds digit of the sum of 1, 12, 1...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[\"Question_pp\"] = [o.outputs[0].text.strip() for o in output_pp]\n",
    "test[\"Question_pp\"] = [o.outputs[0].text.strip() for o in output_pp_test]\n",
    "\n",
    "display(train.head())\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T18:53:37.247059Z",
     "iopub.status.busy": "2025-04-25T18:53:37.246779Z",
     "iopub.status.idle": "2025-04-25T18:53:37.251950Z",
     "shell.execute_reply": "2025-04-25T18:53:37.251181Z",
     "shell.execute_reply.started": "2025-04-25T18:53:37.247039Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual:\n",
      "If $A$, $B$, and $C$ represent three distinct digits from 1 to 9 and they satisfy the following equations, what is the value of the sum $A+B+C$? (In the equation below, $AA$ represents a two-digit number both of whose digits are $A$.) $$A+B=C$$$$AA-B=2\\times C$$$$C\\times B=AA+A$$\n",
      "\n",
      "Paraphrased:\n",
      "Three digits, A, B, and C, each distinct and ranging from 1 to 9, adhere to the following relationships: A + B equals C, AA minus B equals twice C, and C times B equals AA plus A.  Determine the sum of A, B, and C.\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual:\")\n",
    "print(test[\"Question\"].iloc[3])\n",
    "print(\"\\nParaphrased:\")\n",
    "print(test[\"Question_pp\"].iloc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Paraphrased Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T18:52:39.203125Z",
     "iopub.status.busy": "2025-04-25T18:52:39.202371Z",
     "iopub.status.idle": "2025-04-25T18:52:39.405642Z",
     "shell.execute_reply": "2025-04-25T18:52:39.405104Z",
     "shell.execute_reply.started": "2025-04-25T18:52:39.203100Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train.to_csv(\"train_pp.csv\", index=False)\n",
    "test.to_csv(\"test_pp.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11615683,
     "sourceId": 97669,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
